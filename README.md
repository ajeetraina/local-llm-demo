# Local LLM Demo

This project demonstrates how to deploy a language model locally using Docker.

## Note:
- This is a simplified demo that mimics the behavior of a local LLM without actually loading a model
- It uses rule-based responses instead of a real model to ensure it works reliably
- In a real implementation, you would connect this to an actual language model like TinyLlama

## What it works?

This demo shows:

- How to package an AI application in Docker
- The general architecture of a local LLM deployment
- The user interface for interacting with a local model
- How to deploy without external API calls

## Usage

1. Make sure Docker Desktop is installed on your machine
2. Clone the repository and follow the next step.
3. Run the following command:


```
docker compose up -d --build
```

<img width="812" alt="image" src="https://github.com/user-attachments/assets/106c8ffa-9249-4152-ac84-0ec4e29d8d3b" />

